<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">

<head>
<title>Underwater image enhancement with global-local networks and compressed-histogram equalization</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="style.css" type="text/css" media="screen">
<style>
img{
    width:460px;
    height:305px;
}
</style>
</head>


<body leftmargin="40" text="#444444" link="#444444" bgcolor="#FFFFFF">

<div id="top_arrow" style="position: fixed; bottom: 10px; right: 10px;">
<a href="#title">
<img src="../js/top_arrow.jpg" style="border: 0pt none ; width: 26px; height: 32px;"/></a>
</div>

<h2 id="title" class="auto-style1" align="center">Underwater image enhancement with global-local networks and compressed-histogram equalization</h2>

<p class="auto-style7"  align="center">
Xueyang Fu &nbsp;&nbsp; Xiangyong Cao
</p>

<p class="auto-style7"  align="center">
Signal Processing: Image Communication, 2020 
</p>

<p style="text-align: justify;line-height:25px"><strong>Abstract:</strong> Due to the light absorption and scattering, captured underwater images usually contain severe color distortion and contrast reduction. To address the above problems, we combine the merits of deep learning and conventional image enhancement technology to improve the underwater image quality. We first propose a two-branch network to compensate the global distorted color and local reduced contrast, respectively. Adopting this global-local network can greatly ease the learning problem, so that it can be handled by using a lightweight network architecture. To cope with the complex and changeable underwater environment, we then design a compressed-histogram equalization to complement the data-driven deep learning, in which the parameters are fixed after training. The proposed compression strategy is able to generate vivid results without introducing over-enhancement and extra computing burden. Experiments demonstrate that our method significantly outperforms several state-of-the-arts in both qualitative and quantitative qualities.
</p>

<p class="auto-style7"  align="center">
<p><strong>Paper:</strong>
<a  href="https://www.sciencedirect.com/science/article/pii/S0923596520300965" style="color: blue">[pdf]</a>
</p>

<p class="auto-style7"  align="center">
<p><strong>Testing code:</strong>
<a  href="../paper/2020/SPIC/code.zip" style="color: blue">[Tensorflow]</a>
</p>

<p class="auto-style7"  align="center"></p>
<p><strong>Video results:</strong></p>


<p class="auto-style7"  align="center">
<video id='video1' width="500" height="700" controls="controls" autoplay muted>
<source src="../paper/2020/SPIC/video1.mp4">
Your browser does not support the video tag.
</video>&nbsp;&nbsp;&nbsp;  
<video  id='video2' width="500" height="700" controls="controls" preload ="preload">
<source src="../paper/2020/SPIC/video2.mp4">
Your browser does not support the video tag.
</video>&nbsp;&nbsp;&nbsp;   
<video  id='video3' width="500" height="700" controls="controls" preload ="preload">
<source src="../paper/2020/SPIC/video3.mp4">
Your browser does not support the video tag.
</video>
<br>

</p>

</body>


</html>
